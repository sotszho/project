# -*- coding: utf-8 -*-
"""
Created on Thu Jul  5 11:20:56 2018

@author: timso
"""
#%% Directory
import os
os.getcwd()
os.chdir("C:\\Users\\timso\\Documents\\003 Tasks\\1807\\Name matching\\data")  

#%%
""" Libraries """
# Active
import pandas as pd
import numpy as np
import string
import re

import jellyfish
from fuzzywuzzy import fuzz
from fuzzywuzzy import process

#%%
# Others
import pickle
import re
from pkg_resources import Requirement, resource_filename
import csv


""" Real datasets """
raw_table = pd.read_csv('sc_txn_fnl_1805.csv')
test_table = raw_table[['cpty_adj']][0:100]

""" fuzzy string searching """

choices = ["Atlanta Falcons", "New York Jets", "New York Giants", "Dallas Cowboys"]
process.extract("new york jets", choices, limit=2)
    [('New York Jets', 100), ('New York Giants', 78)]
process.extractOne("cowboys", choices)
    ("Dallas Cowboys", 90)

#%%
""" datasets for testing """

# =============================================================================
# bank_list = pd.read_csv('bank_list.csv')
# messy_list = pd.read_csv('messy_list.csv')
# 
# =============================================================================

bank_list = pd.DataFrame({'bank':[              
'Bank of Communications',
'Bank of East Asia',
'CITIC Bank International',
'China Construction Bank (Asia)',
'Chiyu Banking Corporation',
'Chong Hing Bank',
'Citibank (Hong Kong)',
'Dah Sing Bank',
'DBS Bank (Hong Kong)',
'Fubon Bank (Hong Kong)',
'Hang Seng Bank',
'The Hongkong and Shanghai Banking Corporation',
'Industrial and Commercial Bank of China (Asia)',
'Nanyang Commercial Bank',
'OCBC Wing Hang Bank']})


messy_list = pd.DataFrame({'company':[              
'Chan tai man',
'CITIC Bank International',
'',
'   ',
'////',
'abcdefg',
'12345',
'Bank Dah Sing',
'DBS Bank (Hong Kong)',
'Nothing is here!!!',
'Hang   Seng Bank.LTD',
'HSBC, limited',
np.nan,
' no data',
'missing  ']})

#%% Data Exploration/cleaning of messy_list

messy_list.describe
messy_list.columns
messy_list.head()
messy_list.isnull()
messy_list.info()

#%% Exploring the datasets

#bank_list
bank_list.head()

#messy_list'
messy_list.head()
messy_list.info()
messy_list.columns
messy_list["company"][1]

messy_list.isnull()

# Check type for each element
for col in messy_list:
    print(messy_list[col].apply(type))

#%% Parsing
"""
Parsing consists of applying various cleaning techniques

1. standardize datatype and NAs
    a. convert datatype to str
    b. stripping the white spaces and standardizing lettercase to lowercase
    c. convert different 'NA' expressions to np.NaN

2. Standardizing punctuation (e.g., commas must be followed by spaces)
3. Standardizing whitespace (e.g., converting all runs of whitespace to single spaces)
4. Standardizing accented and special characters (e.g., converting accented letters to ASCII equivalents)
5. Check if string contains alphabets; if no, convert to NaN
6. Standardizing legal control terms (e.g., converting "Co." to "Company")
"""

# Checking type for each element
for col in messy_list_c:
    print(messy_list_c[col].apply(type))

#%% Parsing 1
"""
1. standardize datatype and NAs
    a. convert datatype to str
    b. stripping the white spaces and standardizing lettercase to lowercase
    c. convert different 'NA' expressions to np.NaN
"""
# Set str as the default datatype

messy_list_c = messy_list.copy()
messy_list_c['company'] = messy_list_c['company'].astype(str)

# stripping the white spaces and standardizing lettercase to lowercase
messy_list_c.company = messy_list_c.company.str.strip().str.lower()

# converting different 'NA' expressions to np.NaN
messy_list_c = messy_list_c.replace(['nan', 'missing', 'n/a', 'no data'], np.nan)


# End step: convert 'nan' to NaN
messy_list_c = messy_list_c.replace('nan', np.NaN)
messy_list_c['company'].isna()

#%%
"""
2. Standardizing punctuation
    a. commas/dots must be followed by spaces
    b. delete punctuation (only dots and commas here first)
"""
#strip special characters (",", "." and double spaces)

messy_list_c['company']= messy_list_c['company'].str.replace(r"(?<=[.,])(?=[^\s])"," ") # add space if none after . or ,
#messy_list_c['company']= messy_list_c['company'].str.replace(r"\,","") # remove commas
#messy_list_c['company']= messy_list_c['company'].str.replace(r"\.","") # remove dots
for c in string.punctuation:
    messy_list_c['company']= messy_list_c['company'].str.replace(c,"")
    
messy_list_c

#%%
"""
3. Standardizing whitespace (e.g., converting all runs of whitespace to single spaces)
"""
#df.Name = df.Name.replace('\s+', ' ', regex=True)
messy_list_c['company']= messy_list_c['company'].str.replace("\s+", " ")

#%%
"""
4. Standardizing accented and special characters (e.g., converting accented letters to ASCII equivalents)
"""
    # the below code should work however if there is accented characters in the csv to be read
    # the file cannot be read using read_csv in the first place; skip for now
messy_list['Company'] = messy_list['Company'].str.normalize('NFKD')\
                         .str.encode('ascii', errors='ignore')\
                         .str.decode('utf-8')
                         
# ref: https://stackoverflow.com/questions/38969383/fuzzy-string-matching-in-python

#%%
"""
5. Check if string contains alphabets; if no, convert to NaN
"""
test_df = messy_list_c.copy()

def contain_alphabet(x):
    try:
        letters=re.search('[a-z]', x)
        letters[0].isalpha()
        return x
    except:
        return np.nan

test_df['company'] = test_df['company'].map(lambda x: contain_alphabet(x))
test_df

messy_list_c['company'] = messy_list_c['company'].map(lambda x: contain_alphabet(x))
messy_list_c

# drill on letters=re.search('[a-z]', x) vs letters = re.search('[a-zA-Z]+', item)
    
#%%       
# Standardizing legal control terms
# This is just for testing the code; the table is no way exhaustive here;
# should keep a maintained key table in sas/csv

#https://github.com/psolin/cleanco/blob/master/cleanco.py

"""
Basic - hard code the dictionary
"""
#https://stackoverflow.com/questions/49008179/how-to-replace-column-values-with-dictionary-keys-in-pandas

terms_by_type = {
   'corporation': ['incorporated', 'coporation', 'corp', 'inc'],
   'limited': ['ltd', 'llc', 'lltd']}

terms_by_type = {
   'corporation': "incorporated, coporation, corp, inc",
   'limited': "ltd,llc,lltd"}

d = {k: oldk for oldk, oldv in terms_by_type.items() for k in oldv.split(',')}

test_df = messy_list_c.copy()

messy_list_c['company'] = messy_list_c['company'].replace(d, regex=True)
messy_list_c

#%% String Matching
"""
Basic 1 - 100% Match
"""
matched_1=[]
for i in range(len(messy_list_c.company)):
    matched_1.append('No_matches')
    for j in range(len(bank_list.bank)):
        if messy_list_c.company[i] == bank_list.bank[j].lower():
            matched_1[i] = bank_list.bank[j]
            break
matched_1

#messy_list_c['matching'] = matched_1

"""
Basic 2 - 100% match disregarding word order
 - use package or not?
 
"""
matched_2=[]
for i in range(len(messy_list_c.company)):
    matched_2.append('No_matches')
    for j in range(len(bank_list.bank)):
        try:
            if set(messy_list_c.company[i].lower().split()) == set(bank_list.bank[j].lower().split()):
                matched_2[i] = bank_list.bank[j]
                break
        except:
            break

matched_2   

#messy_list_c['matching_2'] = matched_2
#%%
"""
Basic 3 - Phonetic algoithms followed by simple string match
"""
# for bank list
bank_list_metaphone = []
for i in range(len(bank_list)):
    bank_list_metaphone.append('others')
for i in range(len(bank_list)):
    try:
        bank_list_metaphone[i] = jellyfish.metaphone(bank_list['bank'][i])
    except:
        pass

bank_list_metaphone

# for messy list
messy_list_metaphone = []
for i in range(len(messy_list_c)):
    messy_list_metaphone.append('others')
for i in range(len(messy_list_c)):
    try:
        messy_list_metaphone[i] = jellyfish.metaphone(messy_list_c['company'][i])
    except:
        pass

messy_list_metaphone

"""
Basic 3_1 - Phonetic algoithms followed by 100% match
"""

matched_3=[]
for i in range(len(messy_list_metaphone)):
    matched_3.append('No_matches')
    for j in range(len(bank_list_metaphone)):
        try:
            if jellyfish.metaphone(messy_list_c.company[i]) == jellyfish.metaphone(bank_list.bank[j].lower()):
                matched_3[i] = bank_list.bank[j]
        except:
            pass
matched_3

"""
Basic 3_2 - Phonetic algoithms followed by 100% match ignoring word order
"""

matched_3_2=[]
for i in range(len(messy_list_metaphone)):
    matched_3_2.append('No_matches')
    for j in range(len(bank_list_metaphone)):
        try:
            if set(jellyfish.metaphone(messy_list_c.company[i])) == set(jellyfish.metaphone(bank_list.bank[j].lower())):
                matched_3_2[i] = bank_list.bank[j]
        except:
            pass
matched_3_2

#%%
"""
Identify stop_words such as "Limited" "Bank" 

"""
from collections import Counter

def stop_words(word_list):

    counts = Counter(word_list)
    stop_list =[];
    new_word_list=[]
    freq=[]

    for key in counts:
        new_word_list.append(key)
        freq.append(counts[key]/float(sum(counts.values())))

    for i in range(len(freq)):
        if freq[i] > np.percentile(freq,75):
            stop_list.append(new_word_list[i])

    return (new_word_list, stop_list)

def stop_words(word_list, percent):

    counts = Counter(word_list)
    stop_list =[];
    new_word_list=[]
    freq=[]

    for key in counts:
        new_word_list.append(key)
        freq.append(counts[key]/float(sum(counts.values())))

    for i in range(len(freq)):
        if freq[i] > np.percentile(freq,percent):
            stop_list.append(new_word_list[i])

    return (new_word_list, stop_list)

stop_words(['apple', 'orange', 'durian', 'orange'])

stop_words(" ".join(list(bank_list.bank)).split(), 70)

stop_words(" ".join(list(messy_list_c.company.dropna())).split(), 70)


#%%

"""
Basic 4 - Edit distance

The edit distance between two strings S and R is defined to be the minimum
number of character inserts, deletes and changes needed to convert R to S. Given
a text string t of length n, and a pattern string p of length m, informally, the string
edit distance matching problem is to compute the smallest edit distance between p
and substrings of t
"""

"""
formula/ calculation
Output as integer number of edits
Can also be normalized to give a similarity value by the formula

1 - (edit distance / length of the larger of the two strings)
"""
from fuzzywuzzy import fuzz
from fuzzywuzzy import process
from difflib import SequenceMatcher

choices = ["Atlanta Falcons", "New York Jets", "New York Giants", "Dallas Cowboys"]
process.extract("new york jets", choices, limit=2)

#%%
# Ref: https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/
"""
I. String Similarity
using sequencematcher

Return a measure of the sequences' similarity as a float in the range [0, 1]. 
Where T is the total number of elements in both sequences, and  M is the number of matches, 
this is 2.0*M / T.
"""

m = SequenceMatcher(None, "NEW YORK METS", "NEW YORK MEATS") # class difflib.SequenceMatcher(isjunk=None, a='', b='', autojunk=True)
m.ratio()
"""
In this case, T=13+14 = 27; M=13; therefore 26/27 = 0.962962962963
"""
"""
encapsulate it with Fuzzywuzzy
"""

fuzz.ratio("NEW YORK METS", "NEW YORK MEATS")  #96
"""
This standard “string closeness” measurement works fine for very short strings (such as a single word)
and long strings (such as a full book), but not so much for 3-10 word labels. 
The naive approach is far too sensitive to minor differences in word order, 
missing or extra words, and other such issues.
"""
#%%

"""
II. Partial String Similarity
"""
fuzz.ratio("YANKEES", "NEW YORK YANKEES") #61
fuzz.ratio("NEW YORK METS", "NEW YORK YANKEES") #76

"""
This is very related to the case where we match 
1. DAH SING vs DAH SING BANK LIMITED HK
2. DBS BANK LIMITED HK vs DAH SING BANK LIMITED HK

To get around it, we use a heuristic we call “best partial” 
when two strings are of noticeably different lengths (such as the case above). 
If the shorter string is length m, and the longer string is length n, 
we’re basically interested in the score of the best matching length-m substring.

fuzz.ratio("YANKEES", "NEW YOR") ⇒ 14
fuzz.ratio("YANKEES", "EW YORK") ⇒ 28
fuzz.ratio("YANKEES", "W YORK ") ⇒ 28
fuzz.ratio("YANKEES", " YORK Y") ⇒ 28
...
fuzz.ratio("YANKEES", "YANKEES") ⇒ 100

"""
fuzz.partial_ratio("YANKEES", "NEW YORK YANKEES") #100
fuzz.partial_ratio("NEW YORK METS", "NEW YORK YANKEES") #69

#%%

"""
III. Out of order -> Token Sort/ Token Set
"""
fuzz.ratio("New York Mets vs Atlanta Braves", "Atlanta Braves vs New York Mets")
fuzz.partial_ratio("New York Mets vs Atlanta Braves", "Atlanta Braves vs New York Mets")

"""
Substrings aren’t our only problem. We also have to deal with differences in string construction. 
Here is an extremely common pattern, 
    where one seller constructs strings as “<HOME_TEAM> vs <AWAY_TEAM>” 
    and another constructs strings as “<AWAY_TEAM> vs <HOME_TEAM>”
"""
fuzz.token_sort_ratio("New York Mets vs Atlanta Braves", "Atlanta Braves vs New York Mets")

fuzz.token_set_ratio("New York Mets vs Atlanta Braves", "Atlanta Braves vs New York Mets")

#Difference beween token_sort and token_set
fuzz.token_sort_ratio("mariners vs angels", "los angeles angels of anaheim at seattle mariners")
fuzz.token_set_ratio("mariners vs angels", "los angeles angels of anaheim at seattle mariners")

"""
Detailed calculation
1. token_sort_ratio
 step 1 - token and reorder both strings
         t1 - "a am boy I"
         t2 - "a am girl good I noted"
         
 step 2 - compare using simple ratio
"""
t1 = "I am a boy"
t2 = "I am a Noted that good girl"
fuzz.token_sort_ratio(t1, t2)
fuzz.ratio("a am boy I", "a am girl good I noted that")
"""
2. token_set_ratio
    text_1 - "I am a boy"
    text_2 - "Noted that I am a good girl"
 step 1 - t0 = "I am a"
          t1 = "I am a boy"
          t2 = "I am a Noted that good girl"
          fuzz.ratio(t0, t1) ⇒ 90
          fuzz.ratio(t0, t2) ⇒ 46
          fuzz.ratio(t1, t2) ⇒ 50
"""
t0 = "I am a"
t1 = "I am a boy"
t2 = "I am a Noted that good girl"
fuzz.ratio(t0, t1) 75
fuzz.ratio(t0, t2) 36
fuzz.ratio(t1, t2) 43

fuzz.token_set_ratio(t1, t2)

#%%
# try to apply and seems a bit problematic even using token_set_ratio
fuzz.token_set_ratio("DBS BANK LIMITED HK", "DAH BANK LIMITED HK")
fuzz.token_set_ratio("DBS BANK LIMITED HK", "DAH BANK LIIMTED HK")
#%%

"""
Others
https://www.joyofdata.de/blog/comparison-of-string-distance-algorithms/

a SAS paper on fuzzy matching
http://support.sas.com/resources/papers/proceedings17/0881-2017.pdf

ngrams
http://chappers.github.io/web%20micro%20log/2015/04/29/comparison-of-ngram-fuzzy-matching-approaches/

more on tokens
https://www.codediesel.com/nodejs/fuzzy-string-matching-in-nodejs/
"""

# Ngrams - qgrams
test_text = 'I am a boy'

def ngram(text, n=3, pad=True):
    text = text.strip()
    if pad:
        text = " %s " % text
    return set([text[i:i+n] for i in range(len(text)-n+1)])
    
def create_ngram(text1, text2, n=3, pad=True):
    return ngram(text1, n=n, pad=pad), ngram(text2, n=n, pad=pad) 

def qgrams(text1, text2, q=3, pad=True):
    text1, text2 = create_ngram(text1, text2, n=q, pad=pad)
    return len(text1.intersection(text2))

# qgrams counts the numbers of components which match
qgrams("abcde", "abdcde", q=2, pad=False) # 3

#%% using tokens
# ref: https://stackoverflow.com/questions/12821201/what-are-ngram-counts-and-how-to-implement-using-nltk

import nltk
nltk.download('punkt')
from nltk import bigrams
from nltk import trigrams

text="""I am a boy. Jenny is a girl. Jenny does not like to make friends with boys"""
# split the texts into tokens
tokens = nltk.word_tokenize(text)
tokens = [token.lower() for token in tokens if len(token) > 1] #same as unigrams
bi_tokens = bigrams(tokens)
tri_tokens = trigrams(tokens)

# print trigrams count
tri_tokens =list(tri_tokens)
print([(item, tri_tokens.count(item)) for item in sorted(set(tri_tokens))])

#%%
from nltk.corpus import brown
brown.words()

#%% write out to csv and load 
from terms import terms_by_type as type_dict
import json
# Write to file
json.dump(type_dict,open('test.csv',"w"))

# Load from file
da = json.load(open('test.csv',"r"))

#%%

def add_drug_mapping(mydict, mapdict, filename):
    """ This function is used to add drug mappings to 
        the drug dictionary.  For example, if the term
        "benadry" is not found in the dictionary, you can
        add the custom mapping by using the following:
        drugs.add_drug_mapping({"benadryl":"diphenhydramine"})
        Additionally, one might want to map all instances of
        "multi-vitamin" to "vitamin" in which case you would
        use:
        drugs.add_drug_mapping({"multi-vitamin":"vitamin"})
    """
    mydict = json.load(open(filename,"r"))
    for k,v in mapdict.items():
        da[k] = v
    pickle.dump(da, open(filename, "wb"))
    print("The dictionary successfully updated...")

add_drug_mapping(da, {"multi-vitamin":"vitamin"}, 'test.csv')

da

#%% String Matching

#Basic 1 - 100% Match
matched_1=[]
for i in range(len(messy_list_c.Company)):
    matched_1.append('No_matches')
    for j in range(len(bank_list.bank)):
        if messy_list_c.Company[i] == bank_list.Bank[j].lower(): # no lower() on messy_list_c as it is pre-possessed
            matched_1[i] = bank_list.Bank[j]
            break

messy_list['matched_1'] = matched_1

messy_list
#Basic 2 - 100% Match + Case insensitive
matched_2=[]
for i in range(len(messy_list.Company)):
    matched_2.append('No_matches')
    for j in range(len(bank_list.Bank)):
        if messy_list.Company[i].lower() == bank_list.Bank[j].lower():
            matched_2[i] = bank_list.Bank[j]
            break
matched_2
messy_list['matched_2'] = matched_2
messy_list

#Basic 3 - using difflib https://docs.python.org/3.4/library/difflib.html

matched_3=[]
for i in range(len(messy_list.Company)):
    matched_3.append('No_matches')
    matched_3[i] = difflib.get_close_matches(messy_list.Company[i],list(bank_list.Bank), n=1, cutoff=0.1)
matched_3     
        
        
def correct_bank(row):
    accepted = lower(list(bank_list.Bank))
    match = difflib.get_close_matches(row, accepted, n=1, cutoff=0.3)
    return match[0] if match else ''

messy_list['Expense_match'] = list(messy_list['Company'].apply(correct_bank)

# Basic 4 - using fuzzywuzzy

# Basic 5 - using Jaccard Index
# https://stackoverflow.com/questions/11911252/python-jaccard-distance-using-word-intersection-but-not-character-intersection
def DistJaccard(str1, str2):
    str1 = set(str1.split())
    str2 = set(str2.split())
    return float(len(str1 & str2)) / len(str1 | str2)

DistJaccard("DBS Bank Limitedd A", "DBS Bank Limited A")

#%% other references
#http://users.cecs.anu.edu.au/~Peter.Christen/publications/tr-cs-06-02.pdf
#http://users.cecs.anu.edu.au/~Peter.Christen/publications/tr-cs-06-02.pdf
